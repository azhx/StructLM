# StructLM

Structured data like tables, graphs, databases, and other specifications are ubiquitous knowledge sources. Although LLMs have shown reasoning ability on tasks involving plain text, their capability to handle structured data is weak and has not been meaningfully improved. We show that even ChatGPT is weak in handling structured data. To enhance the structured knowledge grounding (SKG) ability of large language models, we build a dataset of 1.1 million instruction following samples over structured data. We adopt the dataset to train \model based off of Code-LLaMA. \model outperforms the previous best single-task structure-grounding models from UnifiedSKG~\citep{DBLP:conf/emnlp/XieW0ZSYWZYWZWL22@UnifiedSKG} on $13$ of $18$ held-in datasets. We also show that \model can generalize well to six diverse unseen knowledge grounding tasks. Such a generalization capability is non-existent in the previous models. Additionally, \model also sets new state-of-the-art on 5 evaluated tasks. Furthermore, we provide a comprehensive ablation to understand the factors that influence the held-in and held-out performance of \model. We release the model weights and dataset, along with relevant code to the community on Github\footnote{\url{https://github.com/azhx/StructLM}}.
